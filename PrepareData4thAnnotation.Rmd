---
title: "PrepareData"
author: "Jean-Francois Chartier"
date: "24 avril 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#install packages
```{r}


library(stringr)
library(magrittr)
library(dplyr)
library(quanteda)


```


#load data
```{r}
#mood.industry.data=readRDS("mood.industry.data4.rds")
load("df_18avril.rda")

```

#Basic data preparetion steps

##create dataframe with unambiguous colnames 
```{r}
mood.industry.data=data.frame(ID_final=df3$ID_final, ID_unique=df3$ID_unique, text=df3$group_sentences, sponsor.feature=df3$SPONSOR2, type.feature=df3$TYPE, act.feature=df3$ACT, label_of_class=df3$codename, text.all.rias=df3$RIAS, stringsAsFactors = F)

training.set=sapply(mood.industry.data$label_of_class, FUN = function(x){
  ifelse(test = x=="NA", FALSE, TRUE)
})
mood.industry.data$training.set=training.set


```


##trim and clean relevant data
```{r}
mood.industry.data$text=mood.industry.data$text%>%str_squish(.)%>%str_to_lower(.)
mood.industry.data$text.all.rias=mood.industry.data$text.all.rias%>%str_squish(.)%>%str_to_lower(.)
mood.industry.data$sponsor.feature=mood.industry.data$sponsor.feature%>%str_squish(.)%>%str_to_lower(.)
mood.industry.data$type.feature=mood.industry.data$type.feature%>%str_squish(.)%>%str_to_lower(.)
mood.industry.data$act.feature=mood.industry.data$act.feature%>%str_squish(.)%>%str_to_lower(.)

```

##correct errors in class label
```{r}
#some class label have an endline marker. We need to erase these
class.label=mood.industry.data$label_of_class %>% stringi::stri_replace_all_fixed(., pattern = "\n", replacement = "", vectorize_all=T)

#correct errors in class label "statment_professional"
class.label[class.label=="statment_professional"]="statement_professional"

mood.industry.data$label_of_class=class.label

```

##correct errors in type.feature
```{r}
#some class label have an endline marker. We need to erase these
type.feature=mood.industry.data$type.feature %>% stringi::stri_replace_all_fixed(., pattern = "theorder", replacement = "the order", vectorize_all=T)

mood.industry.data$type.feature=type.feature
```


##combine class labels
```{r}
statement_institution=(mood.industry.data$label_of_class=="statement_firstnations" | mood.industry.data$label_of_class=="statement_expert" |  mood.industry.data$label_of_class=="statement_institution")
mood.industry.data$label_of_class[statement_institution]="statement_institution"

statement_advocacy=(mood.industry.data$label_of_class=="statement_advocacy" | mood.industry.data$label_of_class=="statement_professional")
mood.industry.data$label_of_class[statement_advocacy]="statement_advocacy"

negative.class=(mood.industry.data$label_of_class=="no_comment" | mood.industry.data$label_of_class=="no_consultation" |  mood.industry.data$label_of_class=="no_statement")

mood.industry.data$label_of_class[negative.class]="is.not.consultation.label"

```


##keep only unique entries
here we keep only entries that share the same text segment, ministry, and class label. The annotators have produced a lot of duplicates during their analysis. We nned to filter them in order to not add biais in the training set
```{r}

id.train.duplicate=mood.industry.data[mood.industry.data$training.set==T, c("text","sponsor.feature", "label_of_class")] %>% duplicated(.)

id.test.duplicated=mood.industry.data[mood.industry.data$training.set==F, c("text","sponsor.feature")] %>% duplicated(.)

mood.industry.train.data=mood.industry.data %>% filter(., training.set==T) %>% filter(., id.train.duplicate==F)

mood.industry.test.data=mood.industry.data %>% filter(., training.set==F) %>% filter(., id.test.duplicated==F)


mood.industry.data=rbind(mood.industry.train.data, mood.industry.test.data)
```

###save mood.industry.data
```{r}
saveRDS(mood.industry.data, "mood.industry.data.4th.rds")
```

```{r}
idTrain=(mood.industry.data$training.set==T)
```


##encode annotation as probability
```{r}
#mood.industry.data=readRDS("mood.industry.data.4th.rds")

#get all unique class label
annotations.prob.labels=mood.industry.data$label_of_class[idTrain]%>%unique(.) #%>%paste0(., "_prob")

#retrieve all identical text segments, and get all different class labels annotators have asigned to them. For example, if a same text segment has been assigned to 3 different labels, we infer that this text segment has a label probability of 0.3 for these 3 class labels and 0 for the others.  
anno.prob.for.obs=sapply(mood.industry.data[idTrain,c("text")], function(x){
  #str(x)
  i=sapply(mood.industry.data[idTrain,c("text")], function(y){
    (x==y)
  })
  #print(i)
  
  pr.of.label=rep(0, length(annotations.prob.labels)) %>%set_names(annotations.prob.labels)
  
  #get how many different labels used
  labels=mood.industry.data$label_of_class[idTrain][i]%>%unique(.)
  p=1/length(labels)
  #print(p)
  
  for (j in 1: length(labels))
  {
    pr.of.label[names(pr.of.label)==labels[j]]=p
  }
  
  #sapply(labels, function(l){
    #print(equals(names(pr.of.label),l))
    #pr.of.label[equals(names(pr.of.label),l)]=p
  #})
  #print(pr.of.label)
  pr.of.label
  #data.frame(annotations.prob.labels)
  #freq.of.dupli$freq[i] %>%data.table::first()
})%>%t(.)

anno.prob.for.obs=anno.prob.for.obs%>%set_colnames(.,paste0(colnames(anno.prob.for.obs), "_prob"))%>%as.data.frame()
```


##assign new annotation
```{r}

all.new.annotation.prob = data.frame(is.not.consultation.prob=numeric(length=nrow(mood.industry.data)), statement.advocacy.prob=numeric(length=nrow(mood.industry.data)), statement.institution.prob=numeric(length=nrow(mood.industry.data)), statement.industry.prob=numeric(length=nrow(mood.industry.data)))

#adding old annotation prob
all.new.annotation.prob$is.not.consultation.prob[idTrain]=anno.prob.for.obs$is.not.consultation.label_prob
all.new.annotation.prob$statement.advocacy.prob[idTrain]=anno.prob.for.obs$statement_advocacy_prob
all.new.annotation.prob$statement.industry.prob[idTrain]=anno.prob.for.obs$statement_industry_prob
all.new.annotation.prob$statement.institution.prob[idTrain]=anno.prob.for.obs$statement_institution_prob


#combine dataframes 
mood.industry.data.with.prob=cbind(mood.industry.data, all.new.annotation.prob)


```

###save
```{r}
saveRDS(mood.industry.data, "mood.industry.data.with.prob.4th.rds")
```


#Text Preprocessing of text segments

##functions used for text modeling
```{r}
#function to project query in latent semantic space
  #'@param matrixV, a term-dim matrix m*k, of m terms previouslly modelleled with a SVD of k latent dimensions
  #'@param sigularValues, the k singular values of the train SVD
  #'@param newData, a new document-term matrix n*m to be projected in the latent space of k dimensions 
predictFromTrainSvdModel<-function(matrixV, singularValues, newData)
{
  call <- match.call()
  tsa <-  newData %*% matrixV %*% solve(diag((singularValues)))
  result <- list(docs_newspace = tsa)
  return (result)
}

#function to create a unit normed vector
normVector <- function(x) 
{
  if(sum(x)==0)
    return (x)
  else 
    return (x / sqrt(sum(x^2)))
  
}
#function to norm many vectors
normRowVectors<-function(m){
  t(apply(m, MARGIN = 1, FUN = function(x) normVector(x)))
}

```

##cleaning textual data
```{r}
#library(stringr)
# tokenisation selon quanteda
preprocesCorpus=stringr::str_replace_all(mood.industry.data$text,"[\r\n]" , "")
#remove all non graphical caracther
preprocesCorpus=stringr::str_replace_all(preprocesCorpus,"[^[:graph:]]", " ")
#remove whitespace
preprocesCorpus=stringr::str_squish(preprocesCorpus)
```

##Tokinization and word filtering
```{r}

preprocesCorpus=quanteda::tokens(x=preprocesCorpus,what="word", remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE,remove_hyphens = TRUE, remove_symbols=TRUE, remove_url = TRUE)

preprocesCorpus=quanteda::tokens_tolower(preprocesCorpus)

myStopWords=unique(c(stopwords("en", source = "smart")))

# filtrer selon un antidictionnaire et singleton
preprocesCorpus=quanteda::tokens_remove(preprocesCorpus, case_insensitive = F, valuetype = "glob", pattern=myStopWords, min_nchar=3)

#lemmatization
preprocesCorpus=sapply(preprocesCorpus, FUN = function(seg)  paste0(textstem::lemmatize_words(seg), collapse = " "))
preprocesCorpus=quanteda::tokens(preprocesCorpus)

#preprocesCorpus = quanteda::tokens_ngrams(preprocesCorpus, n=1:2)

print(c("corpus size after preprocessing : " , length(paste(unlist(preprocesCorpus)))))

print(c("vocabulary size after preprocessing : ", length(unique(paste(unlist(preprocesCorpus))) )))


```

##document*word frequency matrix modeling 
```{r ,cache=T}
#Vectorize documents 
matrix.regcan = quanteda::dfm(x=preprocesCorpus, tolower=FALSE)

#set filter
minDocFreq = 5
maxDocFreq = length(matrix.regcan)*.66

#filter to rare and to frequent words and ngrams 
matrix.regcan<-quanteda::dfm_trim(x=matrix.regcan, min_docfreq = minDocFreq, max_docfreq = maxDocFreq, docfreq_type="count")

# imprimer nombre de dimensions de la matrice
print(paste("nombre de mots differents apres filtrage base sur la frequence documentaire : ", length(matrix.regcan@Dimnames$features)))


```

##norm vectors
```{r}

#unit vector norm
matrix.regcan.norm <- (t(apply(matrix.regcan, MARGIN = 1, FUN = function(x) normVector(x))))
```

##save document*word frequency matrix 
```{r}
saveRDS(matrix.regcan.norm, "wordfrequency.matrix.regcan.norm.4th.rds")
```


##document*word.ngrams frequency matrix modeling
```{r}
preprocesCorpus2=quanteda::tokens(preprocesCorpus, ngrams=2)
#Vectorize documents 
matrix.regcan.ngram = quanteda::dfm(x=preprocesCorpus2, tolower=FALSE)

#set filter
minDocFreq = 5
maxDocFreq = length(matrix.regcan.ngram)*.66

#filter to rare and to frequent words and ngrams 
matrix.regcan.ngram<-quanteda::dfm_trim(x=matrix.regcan.ngram, min_docfreq = minDocFreq, max_docfreq = maxDocFreq, docfreq_type="count")

# imprimer nombre de dimensions de la matrice
print(paste("nombre de mots differents apres filtrage base sur la frequence documentaire : ", length(matrix.regcan.ngram@Dimnames$features)))
```

##norm vectors
```{r}

#unit vector norm
matrix.regcan.ngram.norm <- (t(apply(matrix.regcan.ngram, MARGIN = 1, FUN = function(x) normVector(x))))
```

##save document*ngram frequency matrix 
```{r}
saveRDS(matrix.regcan.ngram.norm, "ngramfrequency.matrix.regcan.norm.4th.rds")
```


##build LSA model
```{r}

#svd.of.regcan.doc=RSpectra::svds(matrix.regcan.norm, 200)
#reduced.svd.regcan.doc=svd.of.regcan.doc$u %*% solve(diag((svd.of.regcan.doc$d)))
#saveRDS(reduced.svd.regcan.doc, "reduced.svd.regcan.doc.rds")
#add colnames
#svd.col.names=sapply(seq(1:ncol(reduced.svd.regcan.doc)), function(x) paste0("latent", x))
#colnames(reduced.svd.regcan.doc)=svd.col.names


svd.from.irba.200=irlba::irlba(matrix.regcan.norm, 200, tol=1e-5)
#change colnames
svd.col.names=sapply(seq(1:length(svd.from.irba.200$d)), function(x) paste0("latent", x))
svd.from.irba.200$feature.names = svd.col.names

```

##save reduced lsa matrix
```{r}
saveRDS(svd.from.irba.200, file = "svd.of.regcan.from.irba.200.4th.rds")
```

#Text preprocessing of all RIAS


##cleaning corpus
```{r}
#library(stringr)
# tokenisation selon quanteda
preprocesRias=stringr::str_replace_all(mood.industry.data$text.all.rias,"[\r\n]" , "")
#remove all non graphical caracther
preprocesRias=stringr::str_replace_all(preprocesRias,"[^[:graph:]]", " ")
#remove whitespace
preprocesRias=stringr::str_squish(preprocesRias)
```

##Tokinization and word filtering
```{r}

preprocesRias=quanteda::tokens(x=preprocesRias,what="word", remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE,remove_hyphens = TRUE, remove_symbols=TRUE, remove_url = TRUE)

preprocesRias=quanteda::tokens_tolower(preprocesRias)

myStopWords=unique(c(stopwords("en", source = "smart")))

preprocesRias=quanteda::tokens_remove(preprocesRias, case_insensitive = F, valuetype = "glob", pattern=myStopWords, min_nchar=3)

#lemmatization
preprocesRias=sapply(preprocesRias, FUN = function(seg)  paste0(textstem::lemmatize_words(seg), collapse = " "))
preprocesRias=quanteda::tokens(preprocesRias)

#preprocesCorpus = quanteda::tokens_ngrams(preprocesCorpus, n=1:2)

print(c("corpus size after preprocessing : " , length(paste(unlist(preprocesRias)))))

print(c("vocabulary size after preprocessing : ", length(unique(paste(unlist(preprocesRias))) )))


```

##document*word frequency matrix modeling 
```{r ,cache=T}
#Vectorize documents 
matrix.context.rias = quanteda::dfm(x=preprocesRias, tolower=FALSE)

#set filter
minDocFreq = 15
maxDocFreq = length(matrix.context.rias)*.66

#filter to rare and to frequent words and ngrams 
matrix.context.rias<-quanteda::dfm_trim(x=matrix.context.rias, min_docfreq = minDocFreq, max_docfreq = maxDocFreq, docfreq_type="count")

# imprimer nombre de dimensions de la matrice
print(paste("nombre de mots differents apres filtrage base sur la frequence documentaire : ", length(matrix.context.rias@Dimnames$features)))


```

##norm vectors
```{r}

#unit vector norm
matrix.context.rias.norm <- (t(apply(matrix.context.rias, MARGIN = 1, FUN = function(x) normVector(x))))
```



##save document*word frequency matrix 
```{r}
saveRDS(matrix.context.rias.norm, "matrix.context.rias.norm.4th.rds")
```

##build LSA model
```{r}

reduced.matrix.context.rias.400=irlba::irlba(matrix.context.rias.norm, 400, tol=1e-5)

#change colnames
svd.col.names=sapply(seq(1:length(reduced.matrix.context.rias.400$d)), function(x) paste0("global.latent.rias.", x))
reduced.matrix.context.rias.400$feature.names = svd.col.names

```


##save 
```{r}
saveRDS(reduced.matrix.context.rias.400, "reduced.matrix.context.rias.400.4th.rds")
```











